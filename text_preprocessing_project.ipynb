{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project1c.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import contractions \t\n",
        "\n",
        "\n",
        "class Text_preprocessor:\n",
        " \"\"\"\n",
        "    A class to preprocess text for NLP Application.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    *\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    expand_contraction(text=\"\"):\n",
        "        returns the expanded text.\n",
        "\n",
        "    remove_special_characters(text=\"\")\n",
        "        returns text with removed emailaddress, special characters and numbers\n",
        "\n",
        "    tokenize(text=\")\n",
        "        returns list of words from text\n",
        "    \n",
        "    removal_stop_words(token=[],language='english')\n",
        "        stop words are derived from  nltk.corpus\n",
        "        returns the list with removed stopwords for english language\n",
        "    \n",
        "    stem_or_lem(token=[],method=\"stemm\")\n",
        "        return the list after lemmitization or stemmization depending upon \n",
        "        method argument\n",
        "    \n",
        "    preprocessed_text(text=\"\")\n",
        "        returns list of words after performing\n",
        "        contaraction, removal of special characters, tokenization, removal of\n",
        "        stop word, stemmization and lemmitization\n",
        "\n",
        "      \n",
        "\n",
        "    \"\"\"\n",
        " def expand_contraction(self, text:str)->str:\n",
        "    '''\n",
        "    Expands the words in text with contractions module.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : str,\n",
        "            text to be expanded\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        text with expanded words \n",
        "    '''\n",
        "    # create an empty list\n",
        "    expanded_words = []    \n",
        "    for word in text.split():\n",
        "      # using contractions.fix to expand the shotened words and removes extra spaces\n",
        "      expanded_words.append(contractions.fix(word))   \n",
        "    expanded_text = ' '.join(expanded_words)\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        " def remove_special_characters(self, text:str)->str:\n",
        "      '''\n",
        "    Removes the email, special character and numbers from text.\n",
        "\n",
        "        Special character includes ! @ # $ & * () + -.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : str\n",
        "            String containing special character\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        String without email, special character and numbers\n",
        "    '''\n",
        "      # remove email if any\n",
        "      txt_email = re.compile(r'[A-Za-z0-9]*@[A-Za-z]*\\.com')\n",
        "      cln_txt = txt_email.sub('', text)\n",
        "      # remove special character and number if any\n",
        "      result = re.sub('[^A-Za-z]+', ' ', cln_txt)      \n",
        "      return result\n",
        "\n",
        " def tokenize(self,text:str)->list:\n",
        "   '''\n",
        "     Tokenize the text  to form list.\n",
        "        Use nltk.word_tokenize.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : str, \n",
        "        text to  tokenize\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        list of tokenized words\n",
        "    '''\n",
        "\n",
        "   nltk_tokens = nltk.word_tokenize(text)\n",
        "   return nltk_tokens\n",
        "\n",
        "\n",
        " def removal_stop_words(self,tokens:list, language:str='english')->list:\n",
        "   '''\n",
        "    Removes the stop words from list.\n",
        "\n",
        "        Use stopwords from nltk.corpus.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token : list\n",
        "            words token\n",
        "        language : str, optional \n",
        "        Language of the words (default is english) \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of words without stop words\n",
        "    '''\n",
        "   stopword_list = nltk.corpus.stopwords.words(language)\n",
        "   tokens_without_sw = [word for word in tokens if not word in stopword_list]\n",
        "   return tokens_without_sw\n",
        "\n",
        " def stem_or_lem(self, tokens:list,method:str)->list:\n",
        "   '''\n",
        "    Perform Stemming or lemmatization.\n",
        "    If the argument method is 'stemm' then performs stemmization, performs\n",
        "    lemmitization if 'lemm' and return tokens for mismatched strings\n",
        "    PorterStemmer  from nltk for stemming\n",
        "    WordNetLemmatizer from nltk for lemmatization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokens : list\n",
        "           list of tokenized words\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        return words after Stemming or Lemmatization\n",
        "    '''\n",
        "   #instance of PorterStemmer \n",
        "   ps = PorterStemmer()\n",
        "   stemmed=[]\n",
        "   lemmed=[]\n",
        "   if method =='stemm':\n",
        "    for w in tokens:\n",
        "        rootWord=ps.stem(w)\n",
        "        stemmed.append(rootWord)\n",
        "    return stemmed\n",
        "   elif method =='lemm':\n",
        "     wordnet_lemmatizer = WordNetLemmatizer()\n",
        "     for w in tokens:\n",
        "        lemm = wordnet_lemmatizer.lemmatize(w)\n",
        "        lemmed.append(lemm)\n",
        "     return lemmed\n",
        "   else:\n",
        "      return tokens\n",
        "\n",
        " def preprocessed_text(self,text:str)->list:\n",
        "    '''\n",
        "    Perfoms all the operation of text preprocessing.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : str, \n",
        "            string to be preprocessed\n",
        "        Returns\n",
        "        -------\n",
        "        returns list of words after performing\n",
        "        contaraction, removal of special characters, tokenization, removal of\n",
        "        stop word, stemmization and lemmitization\n",
        "\n",
        "    '''\n",
        "    exp_text=self.expand_contraction(text)\n",
        "    prune_special=self.remove_special_characters(exp_text)\n",
        "    tokenize_words=self.tokenize(prune_special)\n",
        "    remove_stopwords=self.removal_stop_words(tokenize_words,'english')\n",
        "    stemmed =self.stem_or_lem(remove_stopwords,'stemm')\n",
        "    lemmed =self.stem_or_lem(stemmed,'lemmed')\n",
        "    return lemmed\n",
        "\n",
        "        \n",
        "\n",
        "text = '''This movie made it into one of my top\n",
        " 10 most awful movies. Horrible. I donâ€™t care if it makes 1 million, \n",
        " 10 M , or 100. There wasn't a continuous minute where there wasn't\n",
        "  a fight with one monster or another. There was no chance for any \n",
        "  character development, they were too busy running from one sword \n",
        "  fight to another. I had no emotional attachment ( except to \n",
        "the big bad machine ## that wanted to destroy them). If you \n",
        "disagree with me, you can send your thoughts to idonotcare@leavemealone.com'''  \n",
        "# create an object\n",
        "obj = Text_preprocessor()\n",
        "# perform text pre_processing\n",
        "obj.preprocessed_text(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_ZZINOBghCB",
        "outputId": "61cdb23e-9aae-42ab-c988-8dd8aeb2f382"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.66)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thi',\n",
              " 'movi',\n",
              " 'made',\n",
              " 'one',\n",
              " 'top',\n",
              " 'aw',\n",
              " 'movi',\n",
              " 'horribl',\n",
              " 'I',\n",
              " 'care',\n",
              " 'make',\n",
              " 'million',\n",
              " 'M',\n",
              " 'there',\n",
              " 'continu',\n",
              " 'minut',\n",
              " 'fight',\n",
              " 'one',\n",
              " 'monster',\n",
              " 'anoth',\n",
              " 'there',\n",
              " 'chanc',\n",
              " 'charact',\n",
              " 'develop',\n",
              " 'busi',\n",
              " 'run',\n",
              " 'one',\n",
              " 'sword',\n",
              " 'fight',\n",
              " 'anoth',\n",
              " 'I',\n",
              " 'emot',\n",
              " 'attach',\n",
              " 'except',\n",
              " 'big',\n",
              " 'bad',\n",
              " 'machin',\n",
              " 'want',\n",
              " 'destroy',\n",
              " 'If',\n",
              " 'disagre',\n",
              " 'send',\n",
              " 'thought']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='idonotcare@leavemealone.com'\n",
        "required_output=re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"123\", text)\n",
        "required_output=re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.com', \"123\", text)\n",
        "p = re.compile(r'[A-Za-z0-9]*@[A-Za-z]*\\.com')\n",
        "required_output=p.sub('check', text)\n",
        "print(\"no_emails:\",required_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlDvQy6kj74_",
        "outputId": "86b997ff-4d6b-4a56-af47-a8d215583479"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_emails: check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VDrI6S9fe28b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N2vZ_v3veuPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}