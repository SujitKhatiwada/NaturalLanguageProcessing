{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxkG286eUUq8",
    "outputId": "153fe5e7-d805-4781-91fb-e3d069fe8ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.66)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import contractions \n",
    "from collections import defaultdict\t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "H_ZZINOBghCB"
   },
   "outputs": [],
   "source": [
    "from sys import base_prefix\n",
    "\n",
    "\n",
    "class Text_preprocessor:\n",
    " \"\"\"\n",
    "    A class to preprocess text for NLP Application.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    spec_filtered : list\n",
    "      list with special character removed\n",
    "    rm_stopwrds : list\n",
    "      list with stopwords removed\n",
    "    clean : list\n",
    "      list after all text preprocessing\n",
    "    index_word : dict\n",
    "      dict of indexed words\n",
    "    bag_of_words : df\n",
    "      list of words after preprocessing document\n",
    "    \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    expand_contraction(text=\"\"):\n",
    "        returns the expanded text.\n",
    "\n",
    "    remove_special_characters(text=\"\")\n",
    "        returns text with removed emailaddress, special characters and numbers\n",
    "\n",
    "    tokenize(text=\")\n",
    "        returns list of words from text\n",
    "    \n",
    "    removal_stop_words(token=[],language='english')\n",
    "        stop words are derived from  nltk.corpus\n",
    "        returns the list with removed stopwords for english language\n",
    "    \n",
    "    stem_or_lem(token=[],method=\"stemm\")\n",
    "        return the list after lemmitization or stemmization depending upon \n",
    "        method argument\n",
    "    \n",
    "    preprocessed_text(text=\"\")\n",
    "        returns list of words after performing\n",
    "        contaraction, removal of special characters, tokenization, removal of\n",
    "        stop word, stemmization and lemmitization\n",
    "\n",
    "    bow(document)\n",
    "        returns bag of words for a document(sentence)\n",
    "\n",
    "\n",
    "    get_word_dict\n",
    "        returns key value pair for words in document\n",
    "\n",
    "    computeTF\n",
    "        return dict of computed TF for documents\n",
    "\n",
    "    computeTFIDF\n",
    "        returns dict of IDFS for words in corpus\n",
    "\n",
    "    computeTFIDF\n",
    "        returns dict of computed TFIDF for words in documents\n",
    "    \n",
    "    dict_to_df\n",
    "        returns pandas dataframe representing TFIDS of  list of documents of document\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    " def __init__(self):\n",
    "   self.spec_filtered = []\n",
    "   self.rm_stopwrds = []\n",
    "   self.clean = []\n",
    "   self.index_word = {}\n",
    "   self.bag_of_words = []\n",
    "   self.corpus_words = []\n",
    "   self.clean_document_list = []\n",
    "\n",
    " def expand_contraction(self, text:str)->str:\n",
    "    '''\n",
    "    Expands the words in text with contractions module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str,\n",
    "            text to be expanded\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text with expanded words \n",
    "    '''\n",
    "    # create an empty list\n",
    "    expanded_words = []\n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shotened words and removes extra spaces\n",
    "      expanded_words.append(contractions.fix(word))   \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text.lower()\n",
    "\n",
    "\n",
    " def remove_special_characters(self, text:str)->str:\n",
    "      '''\n",
    "    Removes the email, special character and numbers from text.\n",
    "\n",
    "        Special character includes ! @ # $ & * () + -.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            String containing special character\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        String without email, special character and numbers\n",
    "    '''\n",
    "      # remove email if any\n",
    "      txt_email = re.compile(r'[A-Za-z0-9]*@[A-Za-z]*\\.com')\n",
    "      cln_txt = txt_email.sub('', text)\n",
    "      # remove special character and number if any\n",
    "      self.spec_filtered = re.sub('[^A-Za-z]+', ' ', cln_txt)      \n",
    "      return self.spec_filtered\n",
    "\n",
    " def tokenize(self,text:str)->list:\n",
    "   '''\n",
    "     Tokenize the text  to form list.\n",
    "        Use nltk.word_tokenize.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str, \n",
    "        text to  tokenize\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list of tokenized words\n",
    "    '''\n",
    "\n",
    "   nltk_tokens = nltk.word_tokenize(text)\n",
    "   return nltk_tokens\n",
    "\n",
    "\n",
    " def removal_stop_words(self,tokens:list, language:str='english')->list:\n",
    "   '''\n",
    "    Removes the stop words from list.\n",
    "\n",
    "        Use stopwords from nltk.corpus.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token : list\n",
    "            words token\n",
    "        language : str, optional \n",
    "        Language of the words (default is english) \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of words without stop words\n",
    "    '''\n",
    "   stopword_list = nltk.corpus.stopwords.words(language)\n",
    "   self.rm_stopwrds = [word for word in tokens if not word in stopword_list]\n",
    "   return self.rm_stopwrds\n",
    "\n",
    " def stem_or_lem(self, tokens:list,method:str)->list:\n",
    "   '''\n",
    "    Perform Stemming or lemmatization.\n",
    "    If the argument method is 'stemm' then performs stemmization, performs\n",
    "    lemmitization if 'lemm' and return tokens for mismatched strings\n",
    "    PorterStemmer  from nltk for stemming\n",
    "    WordNetLemmatizer from nltk for lemmatization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : list\n",
    "           list of tokenized words\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        return words after Stemming or Lemmatization\n",
    "    '''\n",
    "   #instance of PorterStemmer \n",
    "   ps = PorterStemmer()\n",
    "   stemmed=[]\n",
    "   lemmed=[]\n",
    "   if method =='stemm':\n",
    "    for w in tokens:\n",
    "        rootWord=ps.stem(w)\n",
    "        stemmed.append(rootWord)\n",
    "    return stemmed\n",
    "   elif method =='lemm':\n",
    "     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "     for w in tokens:\n",
    "        lemm = wordnet_lemmatizer.lemmatize(w)\n",
    "        lemmed.append(lemm)\n",
    "     return lemmed\n",
    "   else:\n",
    "      return tokens\n",
    "\n",
    " def preprocessed_text(self,text:str)->list:\n",
    "    '''\n",
    "    Perfoms all the operation of text preprocessing.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str, \n",
    "            string to be preprocessed\n",
    "        Returns\n",
    "        -------\n",
    "        returns list of words after performing\n",
    "        contaraction, removal of special characters, tokenization, removal of\n",
    "        stop word, stemmization and lemmitization\n",
    "\n",
    "    '''\n",
    "    exp_text=self.expand_contraction(text)\n",
    "    prune_special=self.remove_special_characters(exp_text)\n",
    "    tokenize_words=self.tokenize(prune_special)\n",
    "    remove_stopwords=self.removal_stop_words(tokenize_words,'english')\n",
    "    # stemmed =self.stem_or_lem(remove_stopwords,'stemm')\n",
    "    self.clean =self.stem_or_lem(remove_stopwords,'lemm')\n",
    "    return self.clean\n",
    "\n",
    " def get_word_dict(self,dirty_text, document):\n",
    "   '''\n",
    "    Returns the dict of words after achieved after cleaning\n",
    "\n",
    "      Parameter\n",
    "      ---------\n",
    "      dirty_text : list \n",
    "        list of documents before preprocessing\n",
    "\n",
    "      document : list \n",
    "         list of string [raw_document]\n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "        (key ,value) \n",
    "        Key : word\n",
    "        value : frequency of words in document\n",
    "   '''\n",
    "   \n",
    "  #  all_words = self.get_corpus_words(text)\n",
    "   flatten_list =[item for sublist in self.pre_prep(dirty_text) for item in sublist]\n",
    "   all_words = list(set(flatten_list))\n",
    "   clean_document= self.preprocessed_text(document)\n",
    "\n",
    "  #  words dictionary with value 0\n",
    "   wordDict = dict.fromkeys(all_words, 0) \n",
    "   for word in clean_document:\n",
    "    # count occurence of each words \n",
    "    if word in wordDict:\n",
    "      wordDict[word] += 1\n",
    "   return  wordDict\n",
    "\n",
    " def pre_prep(self, dirty_text:list)-> list:\n",
    "  '''\n",
    "    Returns the list  of documents of documents\n",
    "\n",
    "      Parameter\n",
    "      ---------\n",
    "      dirty_text : list \n",
    "        list of documents before preprocessing\n",
    "\n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "        list of clean documents of clean document\n",
    "   '''\n",
    "  for document in dirty_text:\n",
    "    self.clean_document_list.append(self.preprocessed_text((document)))\n",
    "  return self.clean_document_list\n",
    "\n",
    " def bow(self,dirty_text:list)-> list:\n",
    "   '''\n",
    "    Returns the bag of words in pandas dataframe format\n",
    "\n",
    "      Parameter\n",
    "      ---------\n",
    "      dirty_text : list \n",
    "        list of documents of raw_documents\n",
    "\n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "        bag of words in pandas Df\n",
    "   '''\n",
    "\n",
    "   flatten_list = [item for sublist in self.pre_prep(dirty_text) for item in sublist]\n",
    "   unique_words = list(set(flatten_list))\n",
    "  #  indexing the words from corpus\n",
    "  # first parameter  into keys and second is value(index)\n",
    "   indexed_words= dict(zip(unique_words,range(len(unique_words))))\n",
    "   bow_qrr =[]\n",
    "   for document in self.clean_document_list:\n",
    "    #  create numpy array of of length of corpus\n",
    "     empty_arr = np.zeros(len(unique_words))\n",
    "    #  count the occurence of the each words\n",
    "     for word in document:\n",
    "       empty_arr[indexed_words[word]] += 1\n",
    "     bow_qrr.append(empty_arr)\n",
    "    #  convert array to dataframe\n",
    "   df = pd.DataFrame(bow_qrr,columns =unique_words )\n",
    "   return df\n",
    "    \n",
    " def computeTF(self,dirty_text:list,document)->dict:     \n",
    "      '''\n",
    "      Computes TF for each word in  document\n",
    "\n",
    "      Parameter\n",
    "      ---------\n",
    "        dirty_text : list documents of document\n",
    "\n",
    "          document: list\n",
    "           List of strings(raw_document)\n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        dict\n",
    "        key : word\n",
    "        value : term frequency of the word\n",
    "\n",
    "      '''\n",
    "      tfDict = {}\n",
    "      len_sntn =len(self.preprocessed_text(document))\n",
    "      word_dict = self.get_word_dict(dirty_text,document)\n",
    "      for word, count in word_dict.items():\n",
    "        # term frequency for words in a sentence\n",
    "          tfDict[word] = count/float(len_sntn)\n",
    "      return tfDict\n",
    " \n",
    " def computeIDF(self, dirty_text:list)->dict:\n",
    "    '''\n",
    "      Computes IDFs for all word in doclist\n",
    "\n",
    "      Parameter\n",
    "      ---------\n",
    "        dirty_text : list  \n",
    "          list of pre-processed documents\n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        dict\n",
    "        key : word\n",
    "        value : IDFS for each word\n",
    "\n",
    "      '''\n",
    "    doc_list = []\n",
    "    for doc  in dirty_text:\n",
    "      word_dict = self.get_word_dict(dirty_text, doc)\n",
    "      doc_list.append(word_dict)\n",
    "    \n",
    "    idfDict = {}\n",
    "    N = len(doc_list)\n",
    "    # dict with all words with value 0(template)\n",
    "    idfDict = dict.fromkeys(doc_list[0].keys(), 0)\n",
    "    for doc in doc_list:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                # increase the value if the word exist in doc\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "    return idfDict\n",
    "   \n",
    " def computeTFIDF(self, tf, idfs)->dict:\n",
    "    '''\n",
    "    Computes TFIDFs for all word in documents\n",
    "\n",
    "      Parameter\n",
    "      ---------\n",
    "        tfBow : dict  \n",
    "          Key: word\n",
    "          value: tf of word in document\n",
    "        idfs: dict\n",
    "          key : Word\n",
    "          value: idf of word in document list \n",
    "        \n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        dict\n",
    "        key : word\n",
    "        value : TfIDFS for each word in document\n",
    "    \n",
    "    '''\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    " \n",
    " def dict_to_df(self, text)->list:\n",
    "   '''\n",
    "   Performs all above operations to \n",
    "   Generates pandas dafarame\n",
    "   \n",
    "      Parameter\n",
    "      ---------\n",
    "        text : list of documents (corpus)  \n",
    "                  \n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        Pandas dataframe\n",
    "    '''\n",
    "   arr = []\n",
    "   idfs = self.computeIDF(text)\n",
    "   for doc in text:\n",
    "     tf = self.computeTF(text,doc)\n",
    "     tfidf = self.computeTFIDF(tf,idfs)\n",
    "     arr.append(tfidf)\n",
    "   df = pd.DataFrame(arr)\n",
    "   return df\n",
    "  \n",
    " def pos_identification(self,text)->list:\n",
    "   '''\n",
    "   identifies Part of speech in given text \n",
    "   \n",
    "      Parameter\n",
    "      ---------\n",
    "        text : string   \n",
    "                  \n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        list of tuple with identification\n",
    "    \n",
    "   '''\n",
    "   return nltk.pos_tag(nltk.word_tokenize(self.remove_special_characters(text)))\n",
    "\n",
    " def name_entity_identification(self, text:str, lib='spacy')->list:\n",
    "   '''\n",
    "   identifies name or entity of text in given text \n",
    "   \n",
    "      Parameter\n",
    "      ---------\n",
    "        text : string   \n",
    "                  \n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        list of tuple with identification\n",
    "    \n",
    "   '''\n",
    "  #  use nltk for identification\n",
    "   if lib == 'nltk':\n",
    "     print(\"nltk\")\n",
    "     for chunk in nltk.ne_chunk(self.pos_identification(text)):\n",
    "       if hasattr(chunk, 'label'):\n",
    "         idntfy= ([(chunk.label(), chunk) for c in chunk])\n",
    "      \n",
    "      # use spacy for identification \n",
    "   doc = nlp(obj.remove_special_characters(text))\n",
    "   idntfy = ([(X.text, X.label_) for X in doc.ents])\n",
    "   return ([(X.text, X.label_) for X in doc.ents])\n",
    "\n",
    " def ngram_tokenization(self, text, n=2)->list:\n",
    "   '''\n",
    "   identifies name or entity of text in given text \n",
    "   \n",
    "      Parameter\n",
    "      ---------\n",
    "        text : string   \n",
    "                  \n",
    "\n",
    "      Returns\n",
    "      --------\n",
    "        list of tuple with identification\n",
    "    \n",
    "   '''\n",
    "   n_grams = ngrams(nltk.word_tokenize(obj.remove_special_characters(text)), n)\n",
    "   return [' '.join(grams) for grams in n_grams]\n",
    "  \n",
    "\n",
    "text2 =['This is a good movie.',\n",
    "      'It is a good movie, but you know good is relative.',\n",
    "      'Movie is fun to watch.',\n",
    "      'I had a good relaxing time.',\n",
    "      'The whole cinema experience was good.',\n",
    "      'This is a good cinema.']\n",
    "\n",
    "text3 = '    Jack and jill have made a delicious,dish.Then they started to play some12 game! and jill has attahacd# [a] photo frame to the straight9 wall and swung on sea-saw. She was very happy. After the game, they both went to central London to enjoy some fast food.'\n",
    " \n",
    "\n",
    "obj = Text_preprocessor()\n",
    "\n",
    "\n",
    "# print(\"BOW:\")\n",
    "# print(obj.bow(text2))\n",
    "\n",
    "# print(\"TF for a sentence :\")\n",
    "# print(obj.computeTF(text2, text2[0]))\n",
    "\n",
    "# print(\"IDFS:\")\n",
    "# print(obj.computeIDF(text2))\n",
    "\n",
    "# print(\"TFIDFS:\")\n",
    "# print(obj.dict_to_df(text2))\n",
    "\n",
    "# print(\"Cleaning:\")\n",
    "# print(obj.remove_special_characters(text3))\n",
    "\n",
    "# print(\"POS Identification:\")\n",
    "# print(obj.pos_identification(text3))\n",
    "\n",
    "# print(\"Name Entity identification:\")\n",
    "# print(obj.name_entity_identification(text3))\n",
    "\n",
    "# print(\"N Gram tokenization:\")\n",
    "# obj.ngram_tokenization(text3, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_aOp8_bgPlN",
    "outputId": "03cd9e02-1ea1-4e69-f08b-e920e29f7bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning:\n",
      " Jack and jill have made a delicious dish Then they started to play some game and jill has attahacd a photo frame to the straight wall and swung on sea saw She was very happy After the game they both went to central London to enjoy some fast food \n"
     ]
    }
   ],
   "source": [
    "# clear Special Character and Numbers\n",
    "print(\"Cleaning:\")\n",
    "print(obj.remove_special_characters(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNTLK1DigSRJ",
    "outputId": "d6acdf6b-0f9c-4eba-87ce-059bdd9530da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Identification:\n",
      "[('Jack', 'NNP'), ('and', 'CC'), ('jill', 'NN'), ('have', 'VBP'), ('made', 'VBN'), ('a', 'DT'), ('delicious', 'JJ'), ('dish', 'NN'), ('Then', 'RB'), ('they', 'PRP'), ('started', 'VBD'), ('to', 'TO'), ('play', 'VB'), ('some', 'DT'), ('game', 'NN'), ('and', 'CC'), ('jill', 'NN'), ('has', 'VBZ'), ('attahacd', 'RP'), ('a', 'DT'), ('photo', 'NN'), ('frame', 'NN'), ('to', 'TO'), ('the', 'DT'), ('straight', 'JJ'), ('wall', 'NN'), ('and', 'CC'), ('swung', 'NN'), ('on', 'IN'), ('sea', 'NN'), ('saw', 'VBD'), ('She', 'PRP'), ('was', 'VBD'), ('very', 'RB'), ('happy', 'JJ'), ('After', 'IN'), ('the', 'DT'), ('game', 'NN'), ('they', 'PRP'), ('both', 'DT'), ('went', 'VBD'), ('to', 'TO'), ('central', 'JJ'), ('London', 'NNP'), ('to', 'TO'), ('enjoy', 'VB'), ('some', 'DT'), ('fast', 'JJ'), ('food', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(\"POS Identification:\")\n",
    "print(obj.pos_identification(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taH9KPb_gbJ5",
    "outputId": "dd4513f4-eb9b-43b5-c489-effa367523fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Entity identification:\n",
      "nltk\n",
      "[('Jack', 'PERSON'), ('jill', 'PERSON'), ('London', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Name Entity identification:\")\n",
    "print(obj.name_entity_identification(text3,'nltk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTM4YSpOY6Q4",
    "outputId": "f29620e1-93aa-4ca5-f774-6e792e515259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Entity identification:\n",
      "[('Jack', 'PERSON'), ('jill', 'PERSON'), ('London', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Name Entity identification:\")\n",
    "print(obj.name_entity_identification(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_etxDCsgvoM",
    "outputId": "a84c2545-8537-408c-9df8-4621d5b6d578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Gram tokenization:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Jack and',\n",
       " 'and jill',\n",
       " 'jill have',\n",
       " 'have made',\n",
       " 'made a',\n",
       " 'a delicious',\n",
       " 'delicious dish',\n",
       " 'dish Then',\n",
       " 'Then they',\n",
       " 'they started',\n",
       " 'started to',\n",
       " 'to play',\n",
       " 'play some',\n",
       " 'some game',\n",
       " 'game and',\n",
       " 'and jill',\n",
       " 'jill has',\n",
       " 'has attahacd',\n",
       " 'attahacd a',\n",
       " 'a photo',\n",
       " 'photo frame',\n",
       " 'frame to',\n",
       " 'to the',\n",
       " 'the straight',\n",
       " 'straight wall',\n",
       " 'wall and',\n",
       " 'and swung',\n",
       " 'swung on',\n",
       " 'on sea',\n",
       " 'sea saw',\n",
       " 'saw She',\n",
       " 'She was',\n",
       " 'was very',\n",
       " 'very happy',\n",
       " 'happy After',\n",
       " 'After the',\n",
       " 'the game',\n",
       " 'game they',\n",
       " 'they both',\n",
       " 'both went',\n",
       " 'went to',\n",
       " 'to central',\n",
       " 'central London',\n",
       " 'London to',\n",
       " 'to enjoy',\n",
       " 'enjoy some',\n",
       " 'some fast',\n",
       " 'fast food']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"N Gram tokenization:\")\n",
    "obj.ngram_tokenization(text3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhIxD5ne-6LN",
    "outputId": "093798f7-bccb-4b46-dde0-73bb10b0cc61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('jill', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('made', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('delicious', 'JJ'),\n",
       " ('dish', 'NN'),\n",
       " ('Then', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('started', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('play', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('game', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('jill', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('attahacd', 'RP'),\n",
       " ('a', 'DT'),\n",
       " ('photo', 'NN'),\n",
       " ('frame', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('straight', 'JJ'),\n",
       " ('wall', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('swung', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('sea', 'NN'),\n",
       " ('saw', 'VBD'),\n",
       " ('She', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('very', 'RB'),\n",
       " ('happy', 'JJ'),\n",
       " ('After', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('game', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('both', 'DT'),\n",
       " ('went', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('central', 'JJ'),\n",
       " ('London', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('enjoy', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('fast', 'JJ'),\n",
       " ('food', 'NN')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "nltk.pos_tag(nltk.word_tokenize(obj.remove_special_characters(text3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7GYrZBaBoMT",
    "outputId": "c519bcc0-b041-4fa6-d81b-47590a72aedf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack and jill',\n",
       " 'and jill have',\n",
       " 'jill have made',\n",
       " 'have made a',\n",
       " 'made a delicious',\n",
       " 'a delicious dish',\n",
       " 'delicious dish Then',\n",
       " 'dish Then they',\n",
       " 'Then they started',\n",
       " 'they started to',\n",
       " 'started to play',\n",
       " 'to play some',\n",
       " 'play some game',\n",
       " 'some game and',\n",
       " 'game and jill',\n",
       " 'and jill has',\n",
       " 'jill has attahacd',\n",
       " 'has attahacd a',\n",
       " 'attahacd a photo',\n",
       " 'a photo frame',\n",
       " 'photo frame to',\n",
       " 'frame to the',\n",
       " 'to the straight',\n",
       " 'the straight wall',\n",
       " 'straight wall and',\n",
       " 'wall and swung',\n",
       " 'and swung on',\n",
       " 'swung on sea',\n",
       " 'on sea saw',\n",
       " 'sea saw She',\n",
       " 'saw She was',\n",
       " 'She was very',\n",
       " 'was very happy',\n",
       " 'very happy After',\n",
       " 'happy After the',\n",
       " 'After the game',\n",
       " 'the game they',\n",
       " 'game they both',\n",
       " 'they both went',\n",
       " 'both went to',\n",
       " 'went to central',\n",
       " 'to central London',\n",
       " 'central London to',\n",
       " 'London to enjoy',\n",
       " 'to enjoy some',\n",
       " 'enjoy some fast',\n",
       " 'some fast food']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "# n gram tokenization\n",
    "n_grams = ngrams(nltk.word_tokenize(obj.remove_special_characters(text3)), 3)\n",
    "n_gramed = [' '.join(grams) for grams in n_grams]\n",
    "n_gramed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "VpJBpBk7HIT3",
    "outputId": "95bf30cf-c568-44fe-e465-568da583489b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c9306760c997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_special_characters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'en_core_web_sm' is not defined"
     ]
    }
   ],
   "source": [
    "# test\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(obj.remove_special_characters(text3))\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH4T1_2tXAST"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Group_Project1E_NLP_1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
